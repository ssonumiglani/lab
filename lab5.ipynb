{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Define our actions\n",
    "\n",
    "def action_0():\n",
    "    return np.random.choice([1, 0], p=[0.5, 0.5])\n",
    "\n",
    "def action_1():\n",
    "    return np.random.choice([1, 0], p=[0.6, 0.4])\n",
    "\n",
    "def action_2():\n",
    "    return np.random.choice([1, 0], p=[0.2, 0.8])\n",
    "\n",
    "rewards = [action_0, action_1, action_2]\n",
    "for i in range(10):\n",
    "    print('Pull %d (action_0): reward=%d' % (i, rewards[0]()))\n",
    "    # Simulate action values (Q): expected reward for each action\n",
    "pulls = 100000\n",
    "\n",
    "action_values = []\n",
    "for reward in rewards:\n",
    "    value = [reward() for _ in range(pulls)]  # execute each of the actions 'pulls' times\n",
    "    action_values.append(value)\n",
    "\n",
    "for action, value in enumerate(action_values):\n",
    "    print(\"Action %d: Q(a_%d)=%.2f\" % (action, action, np.mean(value)))\n",
    "    # To simulate the values (V), we need to define a policy\n",
    "# (Value is the expected reward given the policy I'm following)\n",
    "\n",
    "# Define a policy:\n",
    "def policy_random():\n",
    "    '''Returns which action to perform using equal probabilities for each action'''\n",
    "    return np.random.choice([0, 1, 2], p=[1/3, 1/3, 1/3])\n",
    "\n",
    "\n",
    "def policy_better():\n",
    "    ''' A better policy than random: we choose actions 0 and 1 more often than action 2'''\n",
    "    return np.random.choice([0, 1, 2], p=[0.4, 0.5, 0.1])\n",
    "# Simulate Values using the random policy\n",
    "total_reward = 0\n",
    "for pull in range(pulls):\n",
    "    action = policy_random()\n",
    "    total_reward += rewards[action]()\n",
    "print(\"Total reward =\", total_reward)\n",
    "print(\"Average reward: V =\", total_reward/pulls)\n",
    "\n",
    "\n",
    "# Simulate Values using the better policy\n",
    "total_reward = 0\n",
    "for pull in range(pulls):\n",
    "    action = policy_better()\n",
    "    total_reward += rewards[action]()\n",
    "print(\"Total reward =\", total_reward)\n",
    "print(\"Average reward: V =\", total_reward/pulls)\n",
    "\n",
    "\n",
    "\n",
    "# Regret of the better policy\n",
    "V_star = max([np.mean(value) for value in action_values])\n",
    "print(\"V* =\", V_star)\n",
    "\n",
    "total_regret = 0\n",
    "for pull in range(pulls):\n",
    "    total_regret += (V_star - rewards[policy_better()]())\n",
    "print('Regret: I_t = %.2f' % (total_regret/pulls))\n",
    "\n",
    "# Some bandit policies to explore:\n",
    "\n",
    "def policy_greedy(action_values):\n",
    "    '''Always returns the action for which the payoff is highest'''\n",
    "    best_action = np.argmax([np.mean(value) for value in action_values])\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def policy_e_greedy(action_values, epsilon=0.05):\n",
    "    '''We explore with epsilon probability, and choose the best action the rest of the time'''\n",
    "    explore = np.random.choice([1, 0], p=[epsilon, 1-epsilon])\n",
    "    if explore:\n",
    "        # Random action\n",
    "        return policy_random()\n",
    "    else:\n",
    "        # Choose best action\n",
    "        return policy_greedy(action_values)\n",
    "    # Implementing the decaying epsilon-greedy properly requires a class definition so we can store the epsilon values\n",
    "class DecayingEGreedy:\n",
    "    \n",
    "    def __init__(self, epsilon, decay=0.99, lower_bound=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.lower_bound = lower_bound\n",
    "        \n",
    "    def policy(self, action_values):\n",
    "        if self.lower_bound > 0 and self.epsilon > self.lower_bound:\n",
    "            self.epsilon *= self.decay  # update epsilon\n",
    "        explore = np.random.choice([1, 0], p=[self.epsilon, 1-self.epsilon])  # explore vs exploit decision\n",
    "        if explore:\n",
    "            # Random action\n",
    "            return policy_random()\n",
    "        else:\n",
    "            # Choose best action\n",
    "            return policy_greedy(action_values)\n",
    "        \n",
    "        \n",
    "        # Let's test the decaying epsilon-greedy approach\n",
    "agent = DecayingEGreedy(epsilon=0.1, decay=0.99, lower_bound=0.03)\n",
    "\n",
    "# Full problem:\n",
    "action_values = [[], [], []] # initialise values\n",
    "rewards_decaying_e_greedy = []\n",
    "total_reward = 0\n",
    "print('Number of pulls\\t\\tTotal reward\\t\\tV')\n",
    "for pull in range(pulls):\n",
    "    action = agent.policy(action_values)  # choose action according to policy\n",
    "    reward = rewards[action]()  # get reward\n",
    "    action_values[action].append(reward)  # update action_values so we make better decisions down the line\n",
    "    total_reward += reward\n",
    "    if (pull+1) % 1000 == 0:\n",
    "        print('%d\\t\\t\\t%d\\t\\t\\t%.3f' % (pull+1, total_reward, total_reward/pull))\n",
    "        rewards_decaying_e_greedy.append(total_reward/pull)\n",
    "        \n",
    "        \n",
    "        plt.plot(np.arange(1000, pulls+1, step=1000), rewards_decaying_e_greedy)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average reward (V)\")\n",
    "# The average reward is 0.594, which is very close to V* (0.6)!\n",
    "\n",
    "\n",
    "\n",
    "# Implementing the decaying epsilon-greedy properly requires a class definition so we can store the epsilon values\n",
    "class UCB:\n",
    "    \n",
    "    def __init__(self, C=0.5, n_arms=3):\n",
    "        self.C = C\n",
    "        self.pulls = 0\n",
    "        self.counts = np.asarray([0] * n_arms)\n",
    "        \n",
    "    def update_counts(self, arm):\n",
    "        self.pulls += 1\n",
    "        self.counts[arm] += 1\n",
    "        \n",
    "    def policy(self, action_values):\n",
    "        action_values = np.asarray([np.mean(value) for value in action_values])\n",
    "        uncertainty = np.sqrt(np.log(self.pulls) / self.counts)\n",
    "        ucb = action_values + self.C * uncertainty\n",
    "        action = np.argmax(ucb)\n",
    "        self.update_counts(action)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Let's test the decaying epsilon-greedy approach\n",
    "agent = UCB(C=0.5)\n",
    "\n",
    "# Full problem:\n",
    "action_values = [[], [], []] # initialise values\n",
    "total_reward = 0 # reset reward\n",
    "rewards_ucb = []\n",
    "print('Number of pulls\\t\\tTotal reward\\t\\tV')\n",
    "for pull in range(pulls):\n",
    "    action = agent.policy(action_values)  # choose action according to policy\n",
    "    reward = rewards[action]()  # get reward\n",
    "    action_values[action].append(reward)  # update action_values so we make better decisions down the line\n",
    "    total_reward += reward\n",
    "    if (pull+1) % 1000 == 0:\n",
    "        print('%d\\t\\t\\t%d\\t\\t\\t%.3f' % (pull+1, total_reward, total_reward/pull))\n",
    "        rewards_ucb.append(total_reward/pull)\n",
    "        \n",
    "        \n",
    "        plt.plot(np.arange(1000, pulls+1, step=1000), rewards_ucb)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Average reward (V)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
